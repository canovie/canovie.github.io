<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>zookeeper on 青云卷</title>
    <link>https://mydream.ink/tags/zookeeper/</link>
    <description>Recent content in zookeeper on 青云卷</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 02 Nov 2019 09:15:21 +0800</lastBuildDate><atom:link href="https://mydream.ink/tags/zookeeper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>跨 VPC 部署 zookeeper observer</title>
      <link>https://mydream.ink/posts/middleware/zookeeper/deploy-zookeeper-observer-across-vpc/</link>
      <pubDate>Sat, 02 Nov 2019 09:15:21 +0800</pubDate>
      
      <guid>https://mydream.ink/posts/middleware/zookeeper/deploy-zookeeper-observer-across-vpc/</guid>
      <description>场景说明 某云提供商网络技术更新迭代，遗留下了基础网络和 VPC 网络两种网络架构，我们公司目前的服务器还大多都在基础网络中。我在新环境中搭建了 kubernetes 集群，并在上面部署了高可用的 zookeeper 集群。配置中心方案是一个全局方案，公司大大小小的项目陆陆续续都会上，为了便于维护与迁移，我们采用一套环境统一管理。此时，集群内的 zookeeper 已经由 helm 部署完成。
需求  一台已经安装了 docker 和 docker-compose 的位于基础网络服务器（作为 zookeeper observer，为避免单点故障最好备两台及以上) 已经安装好三个结点的 zookeeper 的 kubernetes 的集群  操作流程 1. 基础网络互通 这个步骤具体怎么做我就不提了，每个云服务商可能不同，但并不是什么难点。达到的效果就是将 kubernetes 集群所在的 VPC 和 observer 服务器可以互联互通
2. 向内网暴露 zk 集群相关服务 利用 statefulSet 会为每个 pod 生成的一个特殊的 label，为每一个 zookeeper 的 pod 建立内网 LB 类型的 Service。
这里有一个创建其中一个 svc 的例子：
apiVersion: v1 kind: Service metadata: annotations: # 这里边是与云服务商强相关的一些配置，不具备通用性但仍然具有参考价值，这里需要配置不然默认是公网 LB service.kubernetes.io/qcloud-loadbalancer-clusterid: cls-xxxxxx # 确定 tke 集群 service.</description>
    </item>
    
    <item>
      <title>Kubernetes 中部署高可用的 Zookeeper</title>
      <link>https://mydream.ink/posts/middleware/zookeeper/deploy-zookeeper-ha-in-kubernetes/</link>
      <pubDate>Sat, 02 Nov 2019 08:01:06 +0800</pubDate>
      
      <guid>https://mydream.ink/posts/middleware/zookeeper/deploy-zookeeper-ha-in-kubernetes/</guid>
      <description>本文主要使用 helm chart 进行 zookeeper 的高可用部署。
环境说明 本文使用的是 TKE 托管环境，在该环境中安中了 cbs-csi 的 operator，为了方便持久卷备份，后文中的持久化存储会统一采用。CSI的解决方案在各种环境中都会有，我仅描述 cbs。
  kubernetes 版本：v1.14.3-tke.4
  helm 版本：v2.10.0（TKE控制台支持的版本）
  按步骤安装好的 cbs-csi
 值得说明的是 cbs 存储不可跨可用区挂载，默认情况下先建存储再建 pod 的方式无法满足生产需要，所以才引入的 cbs-csi。但在建立 storageClass 时要注意加入 volumeBindingMode: WaitForFirstConsumer 以保证 pod 完成调度之后再创建卷，从而使得存储不用影响 pod 的调度。sc 参考如下：
 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.beta.kubernetes.io/is-default-class: &amp;#34;true&amp;#34; name: cbs-csi-topo parameters: diskType: CLOUD_PREMIUM provisioner: com.tencent.cloud.csi.cbs reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer   已经部署好了的 prometheus 的监控方案，包括 serviceMonitor 这样的 CRD</description>
    </item>
    
  </channel>
</rss>
